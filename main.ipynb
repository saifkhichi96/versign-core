{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: datasets/Signatures/CustomDataset/\n",
      "\n",
      "analysing TRAINSET ...\n",
      "found 7 authors\n",
      "extracting feature vectors ...\n",
      "\t(007) found 40 signatures (of which genuine: 40)\n",
      "\t(000) found 40 signatures (of which genuine: 40)\n",
      "\t(009) found 40 signatures (of which genuine: 40)\n",
      "\t(008) found 40 signatures (of which genuine: 40)\n",
      "\t(001) found 40 signatures (of which genuine: 40)\n",
      "\t(004) found 40 signatures (of which genuine: 40)\n",
      "\t(005) found 40 signatures (of which genuine: 40)\n",
      "\n",
      "analysing TESTSET ...\n",
      "found 7 authors\n",
      "extracting feature vectors ...\n",
      "\t(007) found 80 signatures (of which genuine: 40)\n",
      "\t(000) found 40 signatures (of which genuine: 40)\n",
      "\t(009) found 80 signatures (of which genuine: 40)\n",
      "\t(008) found 80 signatures (of which genuine: 40)\n",
      "\t(001) found 80 signatures (of which genuine: 40)\n",
      "\t(004) found 40 signatures (of which genuine: 40)\n",
      "\t(005) found 80 signatures (of which genuine: 40)\n",
      "\n",
      "getting predictions...\n",
      "\t007: 85.00% accuracy (FAR: 2.50, FRR: 12.50)\n",
      "\t000: 77.50% accuracy (FAR: 0.00, FRR: 22.50)\n",
      "\t009: 61.25% accuracy (FAR: 7.50, FRR: 31.25)\n",
      "\t008: 52.50% accuracy (FAR: 15.00, FRR: 32.50)\n",
      "\t001: 93.75% accuracy (FAR: 6.25, FRR: 0.00)\n",
      "\t004: 62.50% accuracy (FAR: 0.00, FRR: 37.50)\n",
      "\t005: 58.75% accuracy (FAR: 5.00, FRR: 36.25)\n",
      "\n",
      "STATS\n",
      "\t# of Users: 7\n",
      "\tTestset Size: 480\n",
      "\tAverage Accuracy: 70.18 %\n",
      "\tFalse Positives: 6.04 %\n",
      "\tFalse Negatives: 23.75 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as io\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "from sigver.featurelearning.models import SigNet\n",
    "from sklearn import svm\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class SignatureDataset(Dataset):\n",
    "    \"\"\"Signatures dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir,\n",
    "                 transform=transforms.Compose([transforms.Resize((150, 220)),\n",
    "                                               transforms.ToTensor()])):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with the folders containing signatures of each person.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.people = next(os.walk(root_dir))[1]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.people)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        sample = {'signatures': [], 'names': [], 'id': self.people[idx]}\n",
    "        path = os.path.join(self.root_dir, self.people[idx])\n",
    "        for f in os.listdir(path):\n",
    "            name = os.path.splitext(f)[0]\n",
    "            ext = os.path.splitext(f)[1]\n",
    "            if ext.lower() not in ['.jpg', '.jpeg', '.png', '.tif']:\n",
    "                continue\n",
    "\n",
    "            if name.startswith(\".\"):\n",
    "                continue\n",
    "\n",
    "            im = Image.open(os.path.join(path, f)).convert('1')\n",
    "            if self.transform:\n",
    "                im = self.transform(im)\n",
    "            im = im.view(-1, 150, 220)\n",
    "            sample['signatures'].append(im)\n",
    "            sample['names'].append(name)\n",
    "        \n",
    "        sample['signatures'] = torch.stack(sample['signatures'])\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "def OneClassSVM(x_train, x_test):\n",
    "    # Fit SVM model\n",
    "    clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=2**-11)\n",
    "    clf.fit(x_train)\n",
    "\n",
    "    # Calculate model error\n",
    "    Y_train = clf.predict(x_train)\n",
    "    n_error_train = Y_train[Y_train == -1].size\n",
    "\n",
    "    # Predict results\n",
    "    Y_test = clf.predict(x_test)\n",
    "    Y_prob = np.array(clf.decision_function(x_test))\n",
    "    return Y_test, Y_train, n_error_train, Y_prob\n",
    "\n",
    "# Load the model\n",
    "state_dict, classification_layer, forg_layer = torch.load('models/sabourin/signet.pth')\n",
    "net = SigNet().eval()\n",
    "net.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "root_dir = 'datasets/Signatures/CustomDataset/'\n",
    "print('INPUT:', root_dir)\n",
    "\n",
    "train_dir = root_dir + 'Ref/'\n",
    "test_dir = root_dir + 'Questioned/'\n",
    "\n",
    "datasets = {'trainset': dict(), 'testset': dict()}\n",
    "dirs = [('trainset', train_dir), ('testset', test_dir)]\n",
    "for name, in_dir in dirs:\n",
    "    print('\\nanalysing', name.upper(), '...')\n",
    "    t = transforms.Compose([transforms.Resize((150, 220)),\n",
    "                                     transforms.RandomRotation(2),\n",
    "                                     transforms.ToTensor()])\n",
    "    if name == 'testset':\n",
    "        t = transforms.Compose([transforms.Resize((150, 220)),\n",
    "                                         transforms.ToTensor()])\n",
    "        \n",
    "    dataset = SignatureDataset(in_dir, t)\n",
    "    print('found', len(dataset), 'authors')\n",
    "    print('extracting feature vectors ...')\n",
    "    for i, person in enumerate(dataset):\n",
    "        s = person['signatures']\n",
    "        fn = person['id']\n",
    "#         if not os.path.exists(out_dir):\n",
    "#             os.makedirs(out_dir)\n",
    "\n",
    "        # Extract features\n",
    "        with torch.no_grad(): # We don't need gradients. Inform torch so it doesn't compute them\n",
    "            features = net(s).numpy()\n",
    "#             outfile = os.path.join(out_dir, fn + '.mat')\n",
    "#             io.savemat(outfile, {'feature_vector': features, 'idx': person['names']})\n",
    "            ids = person['names']\n",
    "            X = features\n",
    "            G = 0\n",
    "            y = []\n",
    "            for id in ids:\n",
    "#                 if name == 'trainset':\n",
    "                if 'f' not in id.lower():\n",
    "                    G += 1\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(-1)\n",
    "\n",
    "            print(\"\\t(%s) found %d signatures (of which genuine: %d)\" % (fn, len(s), G))\n",
    "            datasets[name][fn] = {'X': X, 'y': y, 'ids': ids}\n",
    "\n",
    "trainset = datasets['trainset']\n",
    "testset = datasets['testset']\n",
    "\n",
    "right = 0\n",
    "wrong = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "accuracy = 0\n",
    "stats = dict()\n",
    "print('\\ngetting predictions...')\n",
    "for k in trainset.keys():\n",
    "    if k in testset.keys():\n",
    "        X_train = trainset[k]['X']\n",
    "        X_test = testset[k]['X']\n",
    "        y_true = testset[k]['y']\n",
    "        names = testset[k]['ids']\n",
    "        \n",
    "        # Get predictions from OneClassSVM\n",
    "        y_test, y_train, n_error_train, y_prob = OneClassSVM(X_train, X_test)\n",
    "        \n",
    "        # Save stats\n",
    "        pred = y_true==y_test\n",
    "        mistakes = [i for i, x in enumerate(pred) if not x]\n",
    "        \n",
    "        fpos = []\n",
    "        fneg = []\n",
    "        for mistake in mistakes:\n",
    "            if y_true[mistake] == 1 and y_test[mistake] == -1:\n",
    "                fneg.append(names[mistake])\n",
    "            else:\n",
    "                fpos.append(names[mistake])\n",
    "\n",
    "        stats[k] = {\n",
    "            'right': len(pred[pred==True]),\n",
    "            'wrong': len(pred[pred==False]),\n",
    "            'false-pos': fpos,\n",
    "            'false-neg': fneg,\n",
    "            'accuracy': len(pred[pred==True])/len(y_test)*100\n",
    "        }\n",
    "        \n",
    "        right += stats[k]['right']\n",
    "        wrong += stats[k]['wrong']\n",
    "        false_pos += len(stats[k]['false-pos'])\n",
    "        false_neg += len(stats[k]['false-neg'])\n",
    "        accuracy += stats[k]['accuracy']\n",
    "        print(\"\\t%s: %.02f%% accuracy (FAR: %.2f, FRR: %.2f)\" %\n",
    "              (k, stats[k]['accuracy'], \n",
    "               len(stats[k]['false-pos'])/(stats[k]['right']+stats[k]['wrong'])*100,\n",
    "               len(stats[k]['false-neg'])/(stats[k]['right']+stats[k]['wrong'])*100))\n",
    "accuracy /= len(stats.keys())\n",
    "\n",
    "print('\\nSTATS')\n",
    "print(\"\\t# of Users:\", len(stats.keys()))\n",
    "print(\"\\tTestset Size:\", right+wrong)\n",
    "print(\"\\tAverage Accuracy:\", round(accuracy,2), \"%\")\n",
    "print(\"\\tFalse Positives:\", round(false_pos / (right+wrong) * 100, 2), \"%\")\n",
    "print(\"\\tFalse Negatives:\", round(false_neg / (right+wrong) * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
